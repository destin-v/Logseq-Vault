- > RNNs suffer from sequential processing which limits their speed.  RNNs also require attention mechanisms in order to perform well on NLP related tasks.  Both of these problems were solved with the "Attention is All you Need" paper which processes inputs with attention at the input layer.  It also organizes the processing to be parallel like a CNN speeding up the process.
- ![Comparative Study of CNN and RNN for Natural Language Processing.pdf](../assets/Comparative_Study_of_CNN_and_RNN_for_Natural_Language_Processing_1672627227362_0.pdf) [[Convolutional]] [[Natural Language Processing]]